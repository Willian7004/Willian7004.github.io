## 尝试1.58bit量化

Qwen3 2507以及Qwen3 Coder都只有MoE版本，最小的是30b，cpu推理在长上下文速度下降幅度大。由于MoE模型低比特量化效果较好，决定尝试1.58bit量化，对应gguf版本为IQ1_S。

由于长上下文时kv cache占用显存较多，选择把kv cache放到内存，在8g显存的gpu加载39层，prefill速度大约506token/s，短上下文decode速度17.1token/s，12k上下文decode速度3.9token/s。输出与q4版本不一致但仍然可用。

如果选择把kv cache放到显存，由于加载的模型层数减少，短上下文速度下降较大，长上下文速度优势不明显。但在确保36层加载到gpu的情况下仍然可以把kv cache放到显存，能够选择的上下文长度为8192，prefill速度超过4000token/s，满上下文时decode速度约11token/s。减少到35层可把上下文增加到16-20k，但在12k上下文只有4.2token/s，意义不大。

需要注意的是，目前只有Unsloth Dynamic 2.0支持2bit左右的MoE模型量化但只限于知名模型的非微调版本，常规的Imartix量化在MoE模型实用量化级别在3bit左右。