## 我使用LLM转向白嫖策略的原因

Deepseek V2开启了LLM价格战，由于MoE架构和其它成本优化价格降到1元/M输入token、2元/M输出token仍然能保持合理利润，而其它厂商被迫压缩利润率来跟进降价。在这一时期，我的策略是支持开源。如果相应厂商提供的api以开源模型为主则接入。先后接入了Deepseek和Minimax的api。Qwen系列在Qwen3之前的api以Max和Tubro等闭源版本为主力因此改为接入Siliconflow以使用多模态模型。

Deepseek V3相比V2，激活参数量约为1.5倍，总参数量约为3倍，却定了4倍价格。后面相同底模的Deepseek R1以及激活参数量相同的Kimi K2定价又是V3的2倍，不少第三方部署的价格也类似。有消息表示Deepseek R1利润率545%，官方的考虑c端服务以及夜间优惠，实际利润低一些，但按这个价格部署的第三方api利润的确过高了。不过也有便宜一些的，Uniapi凭借渠道做到5.8折优惠，国外有的提供方便宜一些但充值不方便。

我在Deepseek V3到R1时期继续以官方和Siliconflow的api为主。Deepseek R1 0528推出后项目增加且使用Cline等token消耗更大的编程工具，因此改为以Openrouter的免费api为主。由于次数受限，虽然有3个Openrouter账号以及使用部分夜间优惠，但在使用最密集的3天内官方api消耗仍达到约25元。后续编程次数减小，使用Kimi K2时改为在网页版制作原型才基本避免api消耗。另外，智谱的GLM4.5 Flash是Air版本去掉多步搜索得到的，这一模型提供免费api，也比较有优势。

显卡行情也是阻止LLM成本降低的重要因素，这一问题限制了本地部署导致要更加依赖在线LLM，而采购量较大的AI企业应当为此负责。因此，我在日常使用上有过白嫖闭源模型的策略，但近期改用以开源模型为主力的应用为主，实现了从这部分企业获益，同时也避免消耗Openrouter的api次数。
