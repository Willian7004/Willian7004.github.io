## LLM新趋势：MoE与高稀疏度

MoE模型能够节省训练和推理成本，虽然GPT4已使用MoE，但第一个以低成本出名的MoE是Deepseek V2，达到了接近90%的稀疏度。后来的Deepseek V3达到94.5%稀疏度，同样得到较大的成本优势。基于Deepseek V3架构的Kimi K2达到了96.3%稀疏度。

虽然目前高稀疏度模型有提升，但其它MoE模型在稀疏度和成本优化上尚有不足，Qwen3和GLM4.5大约是90%稀疏度并且其它方面的成本优化方法也不及Deepseek。需要进一步优化以降低成本。

端侧模型在侧重CPU/核显推理的情况下，对稀疏度要求比较高。目前Qwen3 30b是90%稀疏度，推理速度仍然不太理想，不过也有第三方微调减少一半激活参数从而获得速度提升。